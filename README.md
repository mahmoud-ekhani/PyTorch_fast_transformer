## Multi-head attention

The Multi-Head Attention mechanism calculates the attention scores for each pair of positions within a sequence. This mechanism features several "attention heads," with each one focusing on different aspects or features of the input sequence, allowing for a more comprehensive understanding of the sequence's context and relationships.

![Multi-head attention](images/multi_head_attention.png)
